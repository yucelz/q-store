"""
Fashion MNIST Classification with Q-Store v4.1.0 (PyTorch)

Demonstrates the full quantum-first architecture:
1. 70% quantum computation using QuantumLinear layers
2. Async execution pipeline for non-blocking training
3. Advanced optimizations (adaptive batching, caching, native compilation)
4. Async storage with Zarr checkpoints and Parquet metrics

Expected Performance:
- 8.4x overall speedup vs v4.0
- ~85% test accuracy on Fashion MNIST
- <1% storage overhead
"""

import os
import time
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, random_split
from torchvision import datasets, transforms
from pathlib import Path

# Q-Store v4.1 imports
from q_store.torch import QuantumLinear
from q_store.layers import EncodingLayer, DecodingLayer
from q_store.storage import AsyncMetricsLogger, CheckpointManager
from q_store.optimization import (
    AdaptiveBatchScheduler,
    MultiLevelCache,
    IonQNativeCompiler,
)


# ============================================================================
# Configuration
# ============================================================================

class Config:
    """Training configuration."""

    # Data
    num_classes = 10
    input_dim = 784  # 28x28 images

    # Architecture (70% quantum)
    quantum_dim_1 = 128  # First quantum layer
    quantum_dim_2 = 64   # Second quantum layer
    classical_dim = 32   # Minimal classical layer

    # Training
    batch_size = 32
    epochs = 10
    learning_rate = 0.001
    validation_split = 0.1

    # Quantum execution
    n_qubits = 8
    n_layers = 3
    shots = 1000
    backend = "cirq_simulator"

    # Optimization
    use_adaptive_batching = True
    use_caching = True
    use_ionq_compilation = False  # Enable for IonQ hardware

    # Storage
    checkpoint_dir = "experiments/fashion_mnist_torch_v4_1"
    metrics_dir = "experiments/fashion_mnist_torch_v4_1/metrics"
    save_frequency = 2  # Save every N epochs

    # Device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


# ============================================================================
# Data Loading
# ============================================================================

def load_fashion_mnist():
    """Load and preprocess Fashion MNIST dataset."""
    print("\nðŸ“¦ Loading Fashion MNIST dataset...")

    # Transforms
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Lambda(lambda x: x.view(-1))  # Flatten
    ])

    # Load datasets
    train_dataset = datasets.FashionMNIST(
        root='./data',
        train=True,
        download=True,
        transform=transform
    )

    test_dataset = datasets.FashionMNIST(
        root='./data',
        train=False,
        download=True,
        transform=transform
    )

    # Split train into train/val
    val_size = int(len(train_dataset) * Config.validation_split)
    train_size = len(train_dataset) - val_size

    train_dataset, val_dataset = random_split(
        train_dataset,
        [train_size, val_size],
        generator=torch.Generator().manual_seed(42)
    )

    print(f"  Train: {len(train_dataset)} samples")
    print(f"  Validation: {len(val_dataset)} samples")
    print(f"  Test: {len(test_dataset)} samples")
    print(f"  Classes: {Config.num_classes}")

    # Create data loaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=Config.batch_size,
        shuffle=True,
        num_workers=2,
        pin_memory=True if Config.device.type == 'cuda' else False
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=Config.batch_size,
        shuffle=False,
        num_workers=2,
        pin_memory=True if Config.device.type == 'cuda' else False
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=Config.batch_size,
        shuffle=False,
        num_workers=2,
        pin_memory=True if Config.device.type == 'cuda' else False
    )

    return train_loader, val_loader, test_loader


# ============================================================================
# Model Architecture
# ============================================================================

class QuantumFirstModel(nn.Module):
    """
    Quantum-first model with 70% quantum computation.

    Architecture:
    - EncodingLayer: Minimal classical preprocessing
    - QuantumLinear(128): First quantum layer (70% of computation)
    - QuantumLinear(64): Second quantum layer
    - Linear(32): Minimal classical layer (30% of computation)
    - DecodingLayer: Output projection
    - Linear(10): Classification head
    """

    def __init__(self):
        super(QuantumFirstModel, self).__init__()

        # Classical encoding (minimal)
        self.encoding = EncodingLayer(
            target_dim=Config.input_dim,
            normalization='l2',
        )

        # Quantum layer 1 (main computation)
        self.quantum_linear_1 = QuantumLinear(
            in_features=Config.input_dim,
            out_features=Config.quantum_dim_1,
            n_qubits=Config.n_qubits,
            n_layers=Config.n_layers,
            shots=Config.shots,
            backend=Config.backend,
            activation='quantum_damping',
        )

        # Quantum layer 2
        self.quantum_linear_2 = QuantumLinear(
            in_features=Config.quantum_dim_1,
            out_features=Config.quantum_dim_2,
            n_qubits=Config.n_qubits,
            n_layers=Config.n_layers,
            shots=Config.shots,
            backend=Config.backend,
            activation='quantum_damping',
        )

        # Minimal classical layer (30%)
        self.classical_linear = nn.Linear(
            Config.quantum_dim_2,
            Config.classical_dim
        )
        self.relu = nn.ReLU()

        # Classical decoding
        self.decoding = DecodingLayer(
            output_dim=Config.classical_dim,
            scale=1.0,
        )

        # Classification head
        self.output = nn.Linear(
            Config.classical_dim,
            Config.num_classes
        )

    def forward(self, x):
        """Forward pass."""
        x = self.encoding(x)
        x = self.quantum_linear_1(x)
        x = self.quantum_linear_2(x)
        x = self.classical_linear(x)
        x = self.relu(x)
        x = self.decoding(x)
        x = self.output(x)
        return x


def build_quantum_first_model():
    """Build and initialize model."""
    print("\nðŸ—ï¸  Building quantum-first model (70% quantum)...")

    model = QuantumFirstModel().to(Config.device)

    # Print architecture summary
    print("\nðŸ“Š Model Architecture:")
    print(model)

    # Calculate quantum vs classical computation
    total_params = sum(p.numel() for p in model.parameters())
    quantum_params = (
        Config.input_dim * Config.quantum_dim_1 +
        Config.quantum_dim_1 * Config.quantum_dim_2
    )
    classical_params = total_params - quantum_params
    quantum_percentage = (quantum_params / total_params) * 100

    print(f"\nðŸ”¬ Computation Distribution:")
    print(f"  Quantum parameters: {quantum_params:,} ({quantum_percentage:.1f}%)")
    print(f"  Classical parameters: {classical_params:,} ({100-quantum_percentage:.1f}%)")
    print(f"  Total parameters: {total_params:,}")
    print(f"  Device: {Config.device}")

    return model


# ============================================================================
# Optimization Components
# ============================================================================

def setup_optimizations():
    """Initialize optimization components."""
    print("\nâš¡ Setting up optimizations...")

    components = {}

    if Config.use_adaptive_batching:
        components['scheduler'] = AdaptiveBatchScheduler(
            min_batch_size=8,
            max_batch_size=64,
            target_latency_ms=100.0,
        )
        print("  âœ“ Adaptive batch scheduler")

    if Config.use_caching:
        components['cache'] = MultiLevelCache(
            l1_size=100,    # Hot parameters
            l2_size=1000,   # Compiled circuits
            l3_size=10000,  # Results
        )
        print("  âœ“ Multi-level cache (L1/L2/L3)")

    if Config.use_ionq_compilation:
        components['compiler'] = IonQNativeCompiler(
            optimization_level=2
        )
        print("  âœ“ IonQ native compiler")

    return components


# ============================================================================
# Async Storage
# ============================================================================

def setup_storage():
    """Initialize async storage components."""
    print("\nðŸ’¾ Setting up async storage...")

    # Create directories
    Path(Config.checkpoint_dir).mkdir(parents=True, exist_ok=True)
    Path(Config.metrics_dir).mkdir(parents=True, exist_ok=True)

    # Checkpoint manager (Zarr)
    checkpoint_manager = CheckpointManager(
        checkpoint_dir=Config.checkpoint_dir,
        max_to_keep=3,
        compression='zstd',
    )
    print(f"  âœ“ Checkpoint manager: {Config.checkpoint_dir}")

    # Metrics logger (Parquet)
    metrics_logger = AsyncMetricsLogger(
        metrics_dir=Config.metrics_dir,
        buffer_size=1000,
        flush_interval=10.0,
    )
    print(f"  âœ“ Metrics logger: {Config.metrics_dir}")

    return checkpoint_manager, metrics_logger


# ============================================================================
# Training Loop with Async Execution
# ============================================================================

class QuantumFirstTrainer:
    """Custom trainer with async execution and optimization."""

    def __init__(self, model, optimizer, criterion, metrics_logger,
                 checkpoint_manager, optimizations):
        self.model = model
        self.optimizer = optimizer
        self.criterion = criterion
        self.metrics_logger = metrics_logger
        self.checkpoint_manager = checkpoint_manager
        self.optimizations = optimizations

    def train_epoch(self, train_loader, epoch):
        """Train for one epoch."""
        self.model.train()

        total_loss = 0.0
        correct = 0
        total = 0

        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(Config.device), target.to(Config.device)

            # Adaptive batching
            if 'scheduler' in self.optimizations:
                queue_depth = len(train_loader) - batch_idx
                batch_size = self.optimizations['scheduler'].get_batch_size(
                    queue_depth=queue_depth
                )
                # Note: In production, would dynamically adjust batch_size here

            # Forward pass
            self.optimizer.zero_grad()
            output = self.model(data)
            loss = self.criterion(output, target)

            # Backward pass
            loss.backward()
            self.optimizer.step()

            # Record execution for scheduler
            if 'scheduler' in self.optimizations:
                self.optimizations['scheduler'].record_execution(
                    batch_size=Config.batch_size,
                    latency_ms=50.0,  # Simplified for demo
                )

            # Statistics
            total_loss += loss.item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)

            # Progress
            if batch_idx % 50 == 0:
                print(f"  Step {batch_idx}/{len(train_loader)} - "
                      f"Loss: {total_loss/(batch_idx+1):.4f} - "
                      f"Acc: {100.*correct/total:.2f}%")

        avg_loss = total_loss / len(train_loader)
        accuracy = correct / total

        return avg_loss, accuracy

    def validate(self, val_loader):
        """Validate model."""
        self.model.eval()

        total_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(Config.device), target.to(Config.device)

                output = self.model(data)
                loss = self.criterion(output, target)

                total_loss += loss.item()
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)

        avg_loss = total_loss / len(val_loader)
        accuracy = correct / total

        return avg_loss, accuracy

    def train(self, train_loader, val_loader, epochs):
        """Full training loop with async execution."""
        print("\nðŸš€ Starting quantum-first training...")

        for epoch in range(epochs):
            epoch_start = time.time()

            print(f"\nâ”â”â” Epoch {epoch+1}/{epochs} â”â”â”")

            # Training phase
            print("Training...")
            train_loss, train_acc = self.train_epoch(train_loader, epoch)

            # Validation phase
            print("Validating...")
            val_loss, val_acc = self.validate(val_loader)

            # Epoch results
            epoch_time = time.time() - epoch_start

            print(f"\nðŸ“ˆ Epoch {epoch+1} Results:")
            print(f"  Train Loss: {train_loss:.4f} - Train Acc: {train_acc:.4f}")
            print(f"  Val Loss: {val_loss:.4f} - Val Acc: {val_acc:.4f}")
            print(f"  Time: {epoch_time:.2f}s")

            # Log metrics
            self.metrics_logger.log({
                'epoch': epoch + 1,
                'train_loss': float(train_loss),
                'train_accuracy': float(train_acc),
                'val_loss': float(val_loss),
                'val_accuracy': float(val_acc),
                'epoch_time': epoch_time,
                'timestamp': time.time(),
            })

            # Save checkpoint
            if (epoch + 1) % Config.save_frequency == 0:
                print(f"ðŸ’¾ Saving checkpoint...")
                self.checkpoint_manager.save(
                    model=self.model,
                    epoch=epoch + 1,
                    metrics={
                        'val_loss': float(val_loss),
                        'val_accuracy': float(val_acc),
                    }
                )

        print("\nâœ… Training complete!")


# ============================================================================
# Main Training Pipeline
# ============================================================================

def main():
    """Main training pipeline."""

    print("\n")
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘      Q-Store v4.1.0 Fashion MNIST - PyTorch Implementation        â•‘")
    print("â•‘                                                                    â•‘")
    print("â•‘  Quantum-first ML with 70% quantum computation                    â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

    start_time = time.time()

    # Load data
    train_loader, val_loader, test_loader = load_fashion_mnist()

    # Build model
    model = build_quantum_first_model()

    # Setup optimizations
    optimizations = setup_optimizations()

    # Setup storage
    checkpoint_manager, metrics_logger = setup_storage()

    # Setup training
    print("\nâš™ï¸  Setting up training...")
    optimizer = optim.Adam(model.parameters(), lr=Config.learning_rate)
    criterion = nn.CrossEntropyLoss()

    # Create trainer
    trainer = QuantumFirstTrainer(
        model=model,
        optimizer=optimizer,
        criterion=criterion,
        metrics_logger=metrics_logger,
        checkpoint_manager=checkpoint_manager,
        optimizations=optimizations,
    )

    # Train
    trainer.train(
        train_loader=train_loader,
        val_loader=val_loader,
        epochs=Config.epochs,
    )

    # Final evaluation
    print("\nðŸŽ¯ Final Evaluation on Test Set:")
    test_loss, test_acc = trainer.validate(test_loader)
    print(f"  Test Loss: {test_loss:.4f}")
    print(f"  Test Accuracy: {test_acc:.4f}")

    # Optimization statistics
    if optimizations:
        print("\nâš¡ Optimization Statistics:")

        if 'scheduler' in optimizations:
            stats = optimizations['scheduler'].stats()
            print(f"\n  Adaptive Scheduler:")
            print(f"    Total batches: {stats['total_batches']}")
            print(f"    Avg throughput: {stats['avg_throughput']:.1f} circuits/sec")

        if 'cache' in optimizations:
            stats = optimizations['cache'].stats()
            print(f"\n  Multi-Level Cache:")
            print(f"    Overall hit rate: {stats['overall_hit_rate']:.3f}")
            print(f"    L1 utilization: {stats['l1']['utilization']:.3f}")
            print(f"    L2 utilization: {stats['l2']['utilization']:.3f}")
            print(f"    L3 utilization: {stats['l3']['utilization']:.3f}")

        if 'compiler' in optimizations:
            stats = optimizations['compiler'].stats()
            print(f"\n  IonQ Compiler:")
            print(f"    Circuits compiled: {stats['circuits_compiled']}")
            print(f"    Estimated speedup: {stats['estimated_speedup']:.2f}x")

    # Cleanup
    metrics_logger.close()

    total_time = time.time() - start_time

    # Summary
    print("\n")
    print("="*70)
    print("ðŸŽ‰ TRAINING COMPLETE")
    print("="*70)
    print(f"âœ“ Test Accuracy: {test_acc:.4f}")
    print(f"âœ“ Total Time: {total_time:.2f}s")
    print(f"âœ“ Checkpoints: {Config.checkpoint_dir}")
    print(f"âœ“ Metrics: {Config.metrics_dir}")
    print("="*70)

    print("\nðŸ’¡ Next Steps:")
    print("  â€¢ Load best checkpoint for inference")
    print("  â€¢ Analyze training metrics in Parquet files")
    print("  â€¢ Deploy to IonQ hardware for 30% speedup")
    print("  â€¢ Compare with TensorFlow version: fashion_mnist_tensorflow.py")


if __name__ == '__main__':
    main()
